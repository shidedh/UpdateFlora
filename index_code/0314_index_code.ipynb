{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import io\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_path = '../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 1.pdf'\n",
    "vol2_path = '../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 2.pdf'\n",
    "vol3_path = '../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 3.pdf'\n",
    "\n",
    "vol1_doc = fitz.open(vol1_path)\n",
    "vol2_doc = fitz.open(vol2_path)\n",
    "vol3_doc = fitz.open(vol3_path)\n",
    "\n",
    "vol1_pages = [vol1_doc[i] for i in range(vol1_doc.page_count)]\n",
    "vol2_pages = [vol2_doc[i] for i in range(vol2_doc.page_count)]\n",
    "vol3_pages = [vol3_doc[i] for i in range(vol3_doc.page_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_char_df = pd.read_pickle(\"../input/char_df/vol1_df.pkl\")\n",
    "vol2_char_df = pd.read_pickle(\"../input/char_df/vol2_df.pkl\")\n",
    "vol3_char_df = pd.read_pickle(\"../input/char_df/vol3_df.pkl\")\n",
    "\n",
    "vol1_index = list(range(616, 639)) #inclusive\n",
    "vol2_index = list(range(703, 725))\n",
    "vol3_index = list(range(555, 583))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DPI = 300\n",
    "mat = fitz.Matrix(TARGET_DPI/ 72, TARGET_DPI/ 72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the columns \n",
    "### & checking if a word is a strict match for the genus / epithet pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epithet_match(row):\n",
    "    return row['word_num'] == 0 and \\\n",
    "           row['word'].isalpha() and \\\n",
    "           row['word'].islower()\n",
    "\n",
    "def genus_match(row):\n",
    "    return row['word_num'] == 0 and \\\n",
    "           row['word'].isalpha() and \\\n",
    "           row['word'][0].isupper() and row['word'][1:].islower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:13<00:00,  1.71it/s]\n",
      "100%|██████████| 22/22 [00:13<00:00,  1.58it/s]\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "#rightmost point of any bounding box:\n",
    "def get_center_x0(vol_char_df, page_num, bias = 30):\n",
    "    \"\"\"WARNING: large bias causes miscatagorization in page number in book\"\"\"\n",
    "    df = vol_char_df[vol_char_df['page_num'] == page_num]\n",
    "    \n",
    "    right_bound = df['line_bbox'].apply(lambda x : x[2]).max() \n",
    "    #leftmost point of any bounding box:\n",
    "    left_bound = df['line_bbox'].apply(lambda x : x[0]).min()\n",
    "\n",
    "    return 0.5*(right_bound + left_bound) - bias\n",
    "\n",
    "\n",
    "def get_col_num(coords, center_x0):\n",
    "    x0, y0, x1, y1 = coords\n",
    "    return int(x0 >= center_x0)\n",
    "\n",
    "\n",
    "all_vol_data = [(vol1_char_df, vol1_index, vol1_doc),\n",
    "                (vol2_char_df, vol2_index, vol2_doc),\n",
    "                (vol3_char_df, vol3_index, vol3_doc)]\n",
    "\n",
    "for vol_char_df ,vol_index, doc in all_vol_data: \n",
    "    #for each volume check if genus pattern / epithet pattern exists within the index part of the book\n",
    "    vol_char_df['genus_index_pat_match'] = vol_char_df.apply(lambda r : r['page_num'] in vol_index and genus_match(r), axis = 1) #does this for whole books which is bad\n",
    "    vol_char_df['epithet_index_pat_match'] = vol_char_df.apply(lambda r : r['page_num'] in vol_index and epithet_match(r), axis = 1) #does this for whole books which is bad\n",
    "    \n",
    "    for page_num in tqdm(vol_index):\n",
    "        center_x0 = get_center_x0(vol_char_df, page_num)\n",
    "        #find center based on x0 coordinate of each line\n",
    "        vol_char_df['col_num'] = vol_char_df['line_bbox'].apply(lambda coords : get_col_num(coords, center_x0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing if col num correctly assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:04<00:00,  5.59it/s]\n",
      "100%|██████████| 22/22 [00:03<00:00,  5.56it/s]\n",
      "100%|██████████| 28/28 [00:04<00:00,  5.97it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index, vol1_doc, \"index_col_vol1\"),\n",
    "                (vol2_char_df, vol2_index, vol2_doc, \"index_col_vol2\"),\n",
    "                (vol3_char_df, vol3_index, vol3_doc, \"index_col_vol3\")]\n",
    "\n",
    "for vol_char_df, vol_index, vol_doc, output_name in all_vol_data:\n",
    "    image_list = []\n",
    "    keep_cols = vol_char_df.columns.difference([\"char_num\", \"char\", \"char_origin\", \"char_bbox\", \"char_x0\", \"char_y0\", \"char_x1\", \"char_y1\", \"pruned_char_x0\", \"pruned_char_y0\", \"pruned_char_x1\", \"pruned_char_y1\"], sort=False).tolist()\n",
    "    for page_num in tqdm(vol_index):\n",
    "        pix_map = vol_doc.get_page_pixmap(page_num,matrix=mat)\n",
    "        image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        temp_df = vol_char_df[vol_char_df[\"page_num\"] == page_num].loc[:, keep_cols].drop_duplicates()\n",
    "\n",
    "        for coord in temp_df[temp_df['col_num'] == 0]['line_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#FF7F50\"), width=5)\n",
    "\n",
    "        for coord in temp_df[temp_df['col_num'] == 1]['line_bbox']:\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#003399\"), width=5)\n",
    "            \n",
    "        image_list.append(image)\n",
    "        #save pages of the volume\n",
    "    image_list[0].save('../output/local/'+output_name+'.pdf' ,save_all=True, append_images=image_list[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genus / epithet flagging \n",
    "flagging pages where number of strict genus or epithet patern matches is less than 3 per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 78.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***FLAGS***\n",
      " number of pages to check: 4\n",
      "  genera\n",
      "\t number of genera: 1, page number: 2, column number: 0\n",
      "\t number of genera: 2, page number: 15, column number: 1\n",
      "\t number of genera: 0, page number: 20, column number: 1\n",
      "\t number of genera: 1, page number: 23, column number: 0\n",
      "  epithets\n",
      "\t number of epithets: 2, page number: 23, column number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 75.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***FLAGS***\n",
      " number of pages to check: 4\n",
      "  genera\n",
      "\t number of genera: 2, page number: 4, column number: 0\n",
      "\t number of genera: 1, page number: 4, column number: 1\n",
      "\t number of genera: 0, page number: 5, column number: 0\n",
      "\t number of genera: 1, page number: 12, column number: 0\n",
      "\t number of genera: 2, page number: 14, column number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 86.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***FLAGS***\n",
      " number of pages to check: 7\n",
      "  genera\n",
      "\t number of genera: 1, page number: 2, column number: 1\n",
      "\t number of genera: 0, page number: 6, column number: 0\n",
      "\t number of genera: 1, page number: 21, column number: 0\n",
      "\t number of genera: 1, page number: 22, column number: 0\n",
      "\t number of genera: 2, page number: 24, column number: 1\n",
      "\t number of genera: 2, page number: 26, column number: 0\n",
      "\t number of genera: 0, page number: 26, column number: 1\n",
      "\t number of genera: 2, page number: 28, column number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index, vol1_doc, \"strickt_match_vol1\"),\n",
    "                (vol2_char_df, vol2_index, vol2_doc, \"strickt_match_vol2\"),\n",
    "                (vol3_char_df, vol3_index, vol3_doc, \"strickt_match_vol3\")]\n",
    "\n",
    "for vol_char_df, vol_index, doc, output_name in all_vol_data: \n",
    "    #for each volume \n",
    "    image_list = []\n",
    "    genus_flag_list = []\n",
    "    epithet_flag_list = []\n",
    "    for page_num in tqdm(vol_index):\n",
    "        genus_db = vol_char_df[(vol_char_df['page_num'] == page_num)\n",
    "                                & (vol_char_df['genus_index_pat_match'] == True)\n",
    "                            ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                            ].drop_duplicates()\n",
    "\n",
    "        epithet_db = vol_char_df[(vol_char_df['page_num'] == page_num) \n",
    "                                & (vol_char_df['epithet_index_pat_match'] == True)\n",
    "                                ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                                ].drop_duplicates()\n",
    "\n",
    "        #genus pattern match flag should check with half page and not entire page:\n",
    "        for col in range(2):\n",
    "            num_genus_col = genus_db[genus_db[\"col_num\"] == col].shape[0]\n",
    "            num_epithet_col = epithet_db[epithet_db[\"col_num\"] == col].shape[0]\n",
    "            if num_genus_col <= 2:\n",
    "                genus_flag_list.append((num_genus_col, page_num - vol_index[0] + 1, col))\n",
    "            if num_epithet_col <= 2:\n",
    "                epithet_flag_list.append((num_epithet_col, page_num - vol_index[0] + 1, col))\n",
    "    num_flag_pages = len(set([g[1] for g in genus_flag_list] + [e[1] for e in epithet_flag_list]))\n",
    "    if num_flag_pages > 0: \n",
    "        print(\"***FLAGS***\")\n",
    "        print(f\" number of pages to check: {num_flag_pages}\")\n",
    "        if genus_flag_list:\n",
    "            print(\"  genera\")\n",
    "            [print(f\"\\t number of genera: {g_flag[0]}, page number: {g_flag[1]}, column number: {g_flag[2]}\") for g_flag in genus_flag_list]\n",
    "        if epithet_flag_list:\n",
    "            print(\"  epithets\")\n",
    "            [print(f\"\\t number of epithets: {e_flag[0]}, page number: {e_flag[1]}, column number: {e_flag[2]}\") for e_flag in epithet_flag_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match  based on coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_coord_match(x, x_ref_left, x_ref_right, margin):\n",
    "    return (x_ref_left - margin <= x[0] and x[0] <= x_ref_left + margin) or (x_ref_right - margin <= x[0] and x[0] <= x_ref_right + margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epithet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 19.76it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 20.05it/s]\n",
      "100%|██████████| 28/28 [00:01<00:00, 23.06it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index),\n",
    "                (vol2_char_df, vol2_index),\n",
    "                (vol3_char_df, vol3_index)]\n",
    "\n",
    "for vol_char_df, vol_index in all_vol_data: \n",
    "    vol_char_df[\"epithet_coord_match\"] = vol_char_df[\"word_bbox\"].apply(lambda x : False)\n",
    "    for page_num in tqdm(vol_index):\n",
    "        margin = 1.25 * vol_char_df[(vol_char_df[\"epithet_index_pat_match\"] == True)][\"char_bbox\"].apply(lambda x : x[2] - x[0]).mean()\n",
    "        epithet_char_df = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"epithet_index_pat_match\"] == True)]\n",
    "        epithet_df = epithet_char_df.loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\", \"char_bbox\"])].drop_duplicates()\n",
    "        page_epithet_2dic = [{}, {}]\n",
    "        \n",
    "        for i in range(epithet_df.shape[0]):\n",
    "            e_index = str(page_num) + \"_\" + str(i)\n",
    "            p0 = epithet_df['word_bbox'].iloc[i]\n",
    "            x_ref = p0[0]\n",
    "            col = epithet_df['col_num'].iloc[i]\n",
    "\n",
    "            ref_neighbors_df = epithet_df[(epithet_df[\"page_num\"] == page_num) & \n",
    "                                          (epithet_df[\"word_bbox\"].apply(lambda x : x_ref - margin <= x[0] and x[0] <= x_ref + margin))]\n",
    "            \n",
    "            num_neighbors = ref_neighbors_df.shape[0]\n",
    "            mean_neighbors = ref_neighbors_df[\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            page_epithet_2dic[col][e_index] = (num_neighbors, mean_neighbors)\n",
    "        \n",
    "        mean_left_epithet = max(page_epithet_2dic[0].values(), default = [-1, -1])[1]\n",
    "        mean_right_epithet = max(page_epithet_2dic[1].values(), default = [-1, -1])[1]\n",
    "\n",
    "        if mean_left_epithet == -1 or mean_right_epithet == -1:\n",
    "            mean_valid_col = max(mean_left_epithet, mean_right_epithet)\n",
    "            vol_char_df.loc[(vol_char_df[\"page_num\"] == page_num) , \"epithet_coord_match\"] = vol_char_df[(vol_char_df[\"page_num\"] == page_num)][\"pruned_word_bbox\"].apply(lambda x : is_coord_match(x, mean_valid_col, mean_valid_col, margin))\n",
    "        elif mean_left_epithet == -1 and mean_right_epithet == -1:\n",
    "            vol_char_df.loc[(vol_char_df[\"page_num\"] == page_num) , \"epithet_coord_match\"] = vol_char_df[(vol_char_df[\"page_num\"] == page_num)][\"pruned_word_bbox\"].apply(lambda x : False)\n",
    "        else: \n",
    "            vol_char_df.loc[(vol_char_df[\"page_num\"] == page_num) , \"epithet_coord_match\"] = vol_char_df[(vol_char_df[\"page_num\"] == page_num)][\"pruned_word_bbox\"].apply(lambda x : is_coord_match(x, mean_left_epithet, mean_right_epithet, margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:04<00:00,  5.49it/s]\n",
      "100%|██████████| 22/22 [00:04<00:00,  5.45it/s]\n",
      "100%|██████████| 28/28 [00:04<00:00,  5.82it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index, vol1_doc, \"epithet_coord_match_pruned_vol1\"),\n",
    "                (vol2_char_df, vol2_index, vol2_doc, \"epithet_coord_match_pruned_vol2\"),\n",
    "                (vol3_char_df, vol3_index, vol3_doc, \"epithet_coord_match_pruned_vol3\")]\n",
    "\n",
    "for vol_char_df, vol_index, doc, output_name in all_vol_data: \n",
    "    #for each volume \n",
    "    image_list = []\n",
    "    \n",
    "    for page_num in tqdm(vol_index):\n",
    "        pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "        image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        epithet_coord_db = vol_char_df[(vol_char_df['page_num'] == page_num) & \n",
    "                                     (vol_char_df['epithet_coord_match'] == True)\n",
    "                            ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                            ].drop_duplicates()\n",
    "\n",
    "        epithet_db = vol_char_df[(vol_char_df['page_num'] == page_num) \n",
    "                                & (vol_char_df['epithet_index_pat_match'] == True)\n",
    "                                ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                                ].drop_duplicates()\n",
    "\n",
    "        #epithet Coord is orange-pinkish, 5\n",
    "        for coord in epithet_coord_db[\"pruned_word_bbox\"] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#FF7F50\"), width=5)\n",
    "\n",
    "        #epithet is blue, 3\n",
    "        for coord in epithet_db['word_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#003399\"), width=3)\n",
    "        image_list.append(image)\n",
    "\n",
    "    #save pages of the volume\n",
    "    image_list[0].save('../output/local/'+output_name+'.pdf' ,save_all=True, append_images=image_list[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genus coord match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 45.68it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 44.01it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 52.69it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index),\n",
    "                (vol2_char_df, vol2_index),\n",
    "                (vol3_char_df, vol3_index)]\n",
    "\n",
    "for vol_char_df, vol_index in all_vol_data: \n",
    "    #genus and not epithet\n",
    "    vol_char_df[\"genus_coord_match\"] = vol_char_df[\"word_bbox\"].apply(lambda x : False)\n",
    "    for page_num in tqdm(vol_index):\n",
    "        margin = 1.25 * vol_char_df[(vol_char_df[\"genus_index_pat_match\"] == True)][\"char_bbox\"].apply(lambda x : x[2] - x[0]).mean()\n",
    "        genus_char_df = vol_char_df[(vol_char_df[\"page_num\"] == page_num) &\n",
    "                                    (vol_char_df[\"genus_index_pat_match\"] == True)]\n",
    "        genus_df = genus_char_df.loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\", \"char_bbox\"])].drop_duplicates()\n",
    "        page_genus_2dic = [{}, {}]\n",
    "        \n",
    "        for i in range(genus_df.shape[0]):\n",
    "            g_index = str(page_num) + \"_\" + str(i)\n",
    "            p0 = genus_df['word_bbox'].iloc[i]\n",
    "            x_ref = p0[0]\n",
    "            col = genus_df['col_num'].iloc[i]\n",
    "\n",
    "            ref_neighbors_df = genus_df[(genus_df[\"page_num\"] == page_num) & \n",
    "                                        (genus_df[\"word_bbox\"].apply(lambda x : x_ref - margin <= x[0] and x[0] <= x_ref + margin))]\n",
    "\n",
    "            num_neighbors = ref_neighbors_df.shape[0]\n",
    "            mean_neighbors = ref_neighbors_df[\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            page_genus_2dic[col][g_index] = (num_neighbors, mean_neighbors)\n",
    "        \n",
    "        mean_left_genus = max(page_genus_2dic[0].values(), default = [-1, -1])[1]\n",
    "        mean_right_genus = max(page_genus_2dic[1].values(), default = [-1, -1])[1]\n",
    "\n",
    "        if mean_left_genus == -1 or mean_right_genus == -1:\n",
    "            mean_valid_col = max(mean_left_genus, mean_right_genus)\n",
    "            vol_char_df.loc[(vol_char_df[\"page_num\"] == page_num) , \"genus_coord_match\"] = vol_char_df[(vol_char_df[\"page_num\"] == page_num)][\"pruned_word_bbox\"].apply(lambda x : is_coord_match(x, mean_valid_col, mean_valid_col, margin))\n",
    "        elif mean_left_genus == -1 and mean_right_genus == -1:\n",
    "            vol_char_df.loc[(vol_char_df[\"page_num\"] == page_num) , \"genus_coord_match\"] = vol_char_df[(vol_char_df[\"page_num\"] == page_num)][\"pruned_word_bbox\"].apply(lambda x : False)\n",
    "        else: \n",
    "            vol_char_df.loc[(vol_char_df[\"page_num\"] == page_num) , \"genus_coord_match\"] = vol_char_df[(vol_char_df[\"page_num\"] == page_num)][\"pruned_word_bbox\"].apply(lambda x : is_coord_match(x, mean_left_genus, mean_right_genus, margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:04<00:00,  5.57it/s]\n",
      "100%|██████████| 22/22 [00:04<00:00,  5.33it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.58it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index, vol1_doc, \"genus_coord_match_vol1\"),\n",
    "                (vol2_char_df, vol2_index, vol2_doc, \"genus_coord_match_vol2\"),\n",
    "                (vol3_char_df, vol3_index, vol3_doc, \"genus_coord_match_vol3\")]\n",
    "\n",
    "for vol_char_df, vol_index, doc, output_name in all_vol_data: \n",
    "    #for each volume \n",
    "    image_list = []\n",
    "\n",
    "    for page_num in tqdm(vol_index):\n",
    "        pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "        image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "\n",
    "        genus_coord_db = vol_char_df[(vol_char_df['page_num'] == page_num) & \n",
    "                                     (vol_char_df['genus_coord_match'] == True)\n",
    "                            ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                            ].drop_duplicates()\n",
    "\n",
    "        epithet_db = vol_char_df[(vol_char_df['page_num'] == page_num) \n",
    "                                & (vol_char_df['epithet_coord_match'] == True)\n",
    "                                ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                                ].drop_duplicates()\n",
    "\n",
    "        #genus Coord is orange-pinkish, 5\n",
    "        for coord in genus_coord_db['word_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#FF7F50\"), width=5)\n",
    "            \n",
    "        # #epithet is red, 3\n",
    "        for coord in epithet_db['word_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#000099\"), width=3)\n",
    "        image_list.append(image)\n",
    "\n",
    "    #save pages of the volume\n",
    "    image_list[0].save('../output/local/'+output_name+'.pdf' ,save_all=True, append_images=image_list[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract potential genus / epithet matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_genus_match(row):\n",
    "    return row['genus_coord_match'] == True and \\\n",
    "           row['epithet_coord_match'] == False and \\\n",
    "           row['word'].isupper() == False and \\\n",
    "           row['word'].isnumeric() == False and \\\n",
    "           row['word'].find(\"Flore\") == -1 #re.search(r\"^Flore$\", row['word'])== None\n",
    "\n",
    "def potential_epithet_match(row):\n",
    "    return row['epithet_coord_match'] == True and \\\n",
    "           row['word'].isupper() == False and \\\n",
    "           row['word'].isnumeric() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_char_df['potential_genus_match'] = vol1_char_df.apply(potential_genus_match, axis = 1)\n",
    "vol1_char_df['potential_epithet_match'] = vol1_char_df.apply(potential_epithet_match, axis = 1)\n",
    "\n",
    "vol2_char_df['potential_genus_match'] = vol2_char_df.apply(potential_genus_match, axis = 1)\n",
    "vol2_char_df['potential_epithet_match'] = vol2_char_df.apply(potential_epithet_match, axis = 1)\n",
    "\n",
    "vol3_char_df['potential_genus_match'] = vol3_char_df.apply(potential_genus_match, axis = 1)\n",
    "vol3_char_df['potential_epithet_match'] = vol3_char_df.apply(potential_epithet_match, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:04<00:00,  5.38it/s]\n",
      "100%|██████████| 22/22 [00:03<00:00,  5.53it/s]\n",
      "100%|██████████| 28/28 [00:04<00:00,  5.79it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index, vol1_doc, \"GE_potential_match_vol1\"),\n",
    "                (vol2_char_df, vol2_index, vol2_doc, \"GE_potential_match_vol2\"),\n",
    "                (vol3_char_df, vol3_index, vol3_doc, \"GE_potential_match_vol3\")]\n",
    "\n",
    "for vol_char_df, vol_index, doc, output_name in all_vol_data: \n",
    "    #for each volume \n",
    "    image_list = []\n",
    "\n",
    "    for page_num in tqdm(vol_index):\n",
    "        pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "        image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "\n",
    "        genus_db = vol_char_df[(vol_char_df['page_num'] == page_num) & \n",
    "                                     (vol_char_df['potential_genus_match'] == True)\n",
    "                            ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                            ].drop_duplicates()\n",
    "\n",
    "        epithet_db = vol_char_df[(vol_char_df['page_num'] == page_num) \n",
    "                                & (vol_char_df['potential_epithet_match'] == True)\n",
    "                                ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                                ].drop_duplicates()\n",
    "\n",
    "        #genus Coord is orange-pinkish, 5\n",
    "        for coord in genus_db['word_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#FF7F50\"), width=5)\n",
    "            \n",
    "        # #epithet is red, 3\n",
    "        for coord in epithet_db['word_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#000099\"), width=3)\n",
    "        image_list.append(image)\n",
    "\n",
    "    #save pages of the volume\n",
    "    image_list[0].save('../output/local/'+output_name+'.pdf' ,save_all=True, append_images=image_list[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infra species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 18.72it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 18.23it/s]\n",
      "100%|██████████| 28/28 [00:01<00:00, 20.92it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index),\n",
    "                (vol2_char_df, vol2_index),\n",
    "                (vol3_char_df, vol3_index)]\n",
    "\n",
    "for vol_char_df, vol_index in all_vol_data: \n",
    "    vol_char_df[\"infra_coord_match\"] = vol_char_df[\"word_bbox\"].apply(lambda x : False)\n",
    "    for page_num in tqdm(vol_index):\n",
    "\n",
    "        margin = 1.25 * vol_char_df[(vol_char_df[\"epithet_coord_match\"] == True) | (vol_char_df[\"genus_coord_match\"] == True)][\"char_bbox\"].apply(lambda x : x[2] - x[0]).mean()\n",
    "        \n",
    "        mean_left_epithet = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"col_num\"] == 0) & (vol_char_df[\"epithet_coord_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "        mean_left_genus = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"col_num\"] == 0) & (vol_char_df[\"genus_coord_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "        if math.isnan(mean_left_genus):\n",
    "            mean_left_genus_all = vol_char_df[(vol_char_df[\"col_num\"] == 0) & (vol_char_df[\"genus_coord_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            mean_left_epithet_all = vol_char_df[(vol_char_df[\"col_num\"] == 0) & (vol_char_df[\"epithet_coord_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            mean_left_tab = mean_left_epithet_all - mean_left_genus_all\n",
    "        else: \n",
    "            mean_left_tab = mean_left_epithet - mean_left_genus\n",
    "        \n",
    "        mean_right_epithet = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"col_num\"] == 1) & (vol_char_df[\"epithet_coord_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "        mean_right_genus = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"col_num\"] == 1) & (vol_char_df[\"genus_coord_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "        if math.isnan(mean_right_genus):\n",
    "            mean_right_genus_all = vol_char_df[(vol_char_df[\"col_num\"] == 1) & (vol_char_df[\"genus_coord_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            mean_right_epithet_all = vol_char_df[(vol_char_df[\"col_num\"] == 1) & (vol_char_df[\"epithet_coord_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            mean_right_tab = mean_right_epithet_all - mean_right_genus_all\n",
    "        else: \n",
    "            mean_right_tab = mean_right_epithet - mean_right_genus\n",
    "\n",
    "        vol_char_df.loc[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"word_num\"] == 0)  , \"infra_coord_match\"] = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"word_num\"] == 0)][\"pruned_word_bbox\"].apply(lambda x : is_coord_match(x, mean_left_epithet + mean_left_tab, mean_right_epithet + mean_right_tab, margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 19.55it/s]\n",
      "100%|██████████| 22/22 [00:01<00:00, 18.09it/s]\n",
      "100%|██████████| 28/28 [00:01<00:00, 20.81it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index),\n",
    "                (vol2_char_df, vol2_index),\n",
    "                (vol3_char_df, vol3_index)]\n",
    "\n",
    "for vol_char_df, vol_index in all_vol_data: \n",
    "    vol_char_df[\"infra_coord_match\"] = vol_char_df[\"word_bbox\"].apply(lambda x : False)\n",
    "    for page_num in tqdm(vol_index):\n",
    "\n",
    "        margin = 1.25 * vol_char_df[(vol_char_df[\"potential_epithet_match\"] == True) | (vol_char_df[\"potential_genus_match\"] == True)][\"char_bbox\"].apply(lambda x : x[2] - x[0]).mean()\n",
    "        \n",
    "        mean_left_epithet = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"col_num\"] == 0) & (vol_char_df[\"potential_epithet_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "        mean_left_genus = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"col_num\"] == 0) & (vol_char_df[\"potential_genus_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "        if math.isnan(mean_left_genus):\n",
    "            mean_left_genus_all = vol_char_df[(vol_char_df[\"col_num\"] == 0) & (vol_char_df[\"potential_genus_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            mean_left_epithet_all = vol_char_df[(vol_char_df[\"col_num\"] == 0) & (vol_char_df[\"potential_epithet_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            mean_left_tab = mean_left_epithet_all - mean_left_genus_all\n",
    "        else: \n",
    "            mean_left_tab = mean_left_epithet - mean_left_genus\n",
    "        \n",
    "        mean_right_epithet = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"col_num\"] == 1) & (vol_char_df[\"potential_epithet_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "        mean_right_genus = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"col_num\"] == 1) & (vol_char_df[\"potential_genus_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "        if math.isnan(mean_right_genus):\n",
    "            mean_right_genus_all = vol_char_df[(vol_char_df[\"col_num\"] == 1) & (vol_char_df[\"potential_genus_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            mean_right_epithet_all = vol_char_df[(vol_char_df[\"col_num\"] == 1) & (vol_char_df[\"potential_epithet_match\"] == True)][\"word_bbox\"].apply(lambda x : x[0]).mean()\n",
    "            mean_right_tab = mean_right_epithet_all - mean_right_genus_all\n",
    "        else: \n",
    "            mean_right_tab = mean_right_epithet - mean_right_genus\n",
    "\n",
    "\n",
    "        vol_char_df.loc[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"word_num\"] == 0)  , \"infra_coord_match\"] = vol_char_df[(vol_char_df[\"page_num\"] == page_num) & (vol_char_df[\"word_num\"] == 0)][\"word_bbox\"].apply(lambda x : is_coord_match(x, mean_left_epithet + mean_left_tab, mean_right_epithet + mean_right_tab, margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_author_match_infra_coord(word):\n",
    "    lower_word = word.lower()\n",
    "    latin_connectives = r\"^\\s?et[\\s|.]?$|^\\s?in[\\s|.]?$|^\\s?non[\\s|.]?$|^\\s?&[\\s|.]?$|^\\s?er[\\s|.]?$|^\\s?nec[\\s|.]?$|^\\s?mult[\\s|.]?$|^\\s?ex[\\s|.]?$|^\\s?fil[\\s|.]?$\"\n",
    "    infra_symbols = r\"^var[\\s|.|\\b]?$|^subsp[\\s|.|\\b]?$|^ssp[\\s|.|\\b]?$|^spp[\\s|.|\\b]?$|^x[\\s|.|\\b]?$|^×[\\s|.|\\b]?$\"\n",
    "    is_latin_connectives = re.search(latin_connectives, word) != None\n",
    "    is_infra_symbol = re.search(infra_symbols, lower_word) != None\n",
    "    return (not is_infra_symbol) and (word[0].isupper() or is_latin_connectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_author_match_infra_coord(\"fil.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index),\n",
    "                (vol2_char_df, vol2_index),\n",
    "                (vol3_char_df, vol3_index)]\n",
    "for vol_char_df, _ in all_vol_data:\n",
    "    vol_char_df[\"potential_infra_match\"] = (vol_char_df[\"infra_coord_match\"] == True) & (vol_char_df['word'].apply(potential_author_match_infra_coord) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_infra_symbols(word):\n",
    "    infra_symbols = r\"^var[\\s|.|\\b]?$|^subsp[\\s|.|\\b]?$|^ssp[\\s|.|\\b]?$|^spp[\\s|.|\\b]?$|^x[\\s|.|\\b]?$|^×[\\s|.|\\b]?$\"\n",
    "    return re.search(infra_symbols, word) != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:25<00:00,  1.11it/s]\n",
      "100%|██████████| 22/22 [00:23<00:00,  1.06s/it]\n",
      "100%|██████████| 23/23 [00:22<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "all_vol_data = [(vol1_char_df, vol1_index, vol1_doc, \"potential_infra_match_vol1\"),\n",
    "                (vol2_char_df, vol2_index, vol2_doc, \"potential_infra_match_vol2\"),\n",
    "                (vol3_char_df, vol3_index, vol3_doc, \"potential_infra_match_vol3\")][::-1]\n",
    "\n",
    "for vol_char_df, vol_index, doc, output_name in all_vol_data: \n",
    "    #for each volume \n",
    "    image_list = []\n",
    "\n",
    "    for page_num in tqdm(vol_index):\n",
    "        pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "        image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "\n",
    "        infra_coord_db = vol_char_df[(vol_char_df['page_num'] == page_num) & \n",
    "                                     (vol_char_df['infra_coord_match'] == True)\n",
    "                            ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                            ].drop_duplicates()\n",
    "\n",
    "        infra_db = vol_char_df[(vol_char_df['page_num'] == page_num) \n",
    "                                & (vol_char_df['potential_infra_match'] == True)\n",
    "                                ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                                ].drop_duplicates()\n",
    "\n",
    "        with_infra_symbols = vol_char_df[(vol_char_df['page_num'] == page_num) &\n",
    "                                         (vol_char_df['infra_coord_match'] == True) & \n",
    "                                         (vol_char_df['word'].apply(has_infra_symbols) == True)\n",
    "                                        ].loc[:,~vol_char_df.columns.isin([\"char_num\", \"char\", \"char_origin\",\t\"char_bbox\"])\n",
    "                                        ].drop_duplicates()\n",
    "\n",
    "        #genus Coord is orange-pinkish, 5\n",
    "        for coord in infra_coord_db['word_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0-5, y0-5, x1+5, y1+5), fill=None, outline=ImageColor.getrgb(\"#003399\"), width=7)\n",
    "\n",
    "        for coord in infra_db['word_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0-3, y0-3, x1+3, y1+3), fill=None, outline=ImageColor.getrgb(\"#FF7F50\"), width=5)\n",
    "            \n",
    "        # #epithet is red, 3\n",
    "        for coord in with_infra_symbols['word_bbox'] :\n",
    "            x0, y0, x1, y1 = [f*TARGET_DPI/ 72 for f in coord]\n",
    "            draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(\"#990000\"), width=3)\n",
    "\n",
    "        image_list.append(image)\n",
    "\n",
    "    #save pages of the volume\n",
    "    image_list[0].save('../output/local/'+output_name+'.pdf' ,save_all=True, append_images=image_list[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for author matching \n",
    "to detect anamolies in epithet and infra indentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [page_num, word]\n",
       "Index: []"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol1_char_df[(vol1_char_df['potential_infra_match'] == True) & (vol1_char_df['word'].apply(has_infra_symbols) == False)][[\"page_num\", \"word\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [page_num, word]\n",
       "Index: []"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol2_char_df[(vol2_char_df['potential_infra_match'] == True) & (vol2_char_df['word'].apply(has_infra_symbols) == False)][[\"page_num\", \"word\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1559345</th>\n",
       "      <td>557</td>\n",
       "      <td>(3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559874</th>\n",
       "      <td>557</td>\n",
       "      <td>f.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561502</th>\n",
       "      <td>558</td>\n",
       "      <td>fa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566359</th>\n",
       "      <td>560</td>\n",
       "      <td>deris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570483</th>\n",
       "      <td>562</td>\n",
       "      <td>cock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576678</th>\n",
       "      <td>565</td>\n",
       "      <td>f.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578491</th>\n",
       "      <td>566</td>\n",
       "      <td>picha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581443</th>\n",
       "      <td>567</td>\n",
       "      <td>adoxifolium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582167</th>\n",
       "      <td>568</td>\n",
       "      <td>fil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584378</th>\n",
       "      <td>569</td>\n",
       "      <td>yar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587338</th>\n",
       "      <td>570</td>\n",
       "      <td>f.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589927</th>\n",
       "      <td>571</td>\n",
       "      <td>f.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592932</th>\n",
       "      <td>573</td>\n",
       "      <td>f.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594835</th>\n",
       "      <td>574</td>\n",
       "      <td>turcicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596384</th>\n",
       "      <td>574</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598587</th>\n",
       "      <td>576</td>\n",
       "      <td>berlain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         page_num         word\n",
       "1559345       557           (3\n",
       "1559874       557           f.\n",
       "1561502       558           fa\n",
       "1566359       560       deris \n",
       "1570483       562         cock\n",
       "1576678       565           f.\n",
       "1578491       566       picha \n",
       "1581443       567  adoxifolium\n",
       "1582167       568        fil. \n",
       "1584378       569         yar.\n",
       "1587338       570           f.\n",
       "1589927       571           f.\n",
       "1592932       573           f.\n",
       "1594835       574     turcicus\n",
       "1596384       574            y\n",
       "1598587       576      berlain"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol3_char_df[(vol3_char_df['potential_infra_match'] == True) & (vol3_char_df['word'].apply(has_infra_symbols) == False)][[\"page_num\", \"word\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_author_match_epithet_coord(word):\n",
    "    latin_connectives = r\"^\\s?et[\\s|.]?$|^\\s?in[\\s|.]?$|^\\s?non[\\s|.]?$|^\\s?&[\\s|.]?$|^\\s?er[\\s|.]?$|^\\s?nec[\\s|.]?$|^\\s?mult[\\s|.]?$|^\\s?ex[\\s|.]?$|^\\s?fil[\\s|.]?$|^\\s?f[\\s|.]?$\"\n",
    "    is_latin_connectives = re.search(latin_connectives, word) != None\n",
    "    is_hybrid = word == \"X\"\n",
    "    return is_latin_connectives or (word[0].isupper() and (not is_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1734317</th>\n",
       "      <td>624</td>\n",
       "      <td>J.d,IlLIlU.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753028</th>\n",
       "      <td>633</td>\n",
       "      <td>Phoenicia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753527</th>\n",
       "      <td>633</td>\n",
       "      <td>Syriacus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755122</th>\n",
       "      <td>634</td>\n",
       "      <td>Jilicaulis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         page_num         word\n",
       "1734317       624  J.d,IlLIlU.\n",
       "1753028       633    Phoenicia\n",
       "1753527       633     Syriacus\n",
       "1755122       634   Jilicaulis"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol1_char_df[(vol1_char_df['potential_epithet_match'] == True) & (vol1_char_df['word'].apply(potential_author_match_epithet_coord))][[\"page_num\", \"word\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1915513</th>\n",
       "      <td>706</td>\n",
       "      <td>Hbanoticus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922530</th>\n",
       "      <td>710</td>\n",
       "      <td>Hppii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937158</th>\n",
       "      <td>716</td>\n",
       "      <td>Ma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         page_num        word\n",
       "1915513       706  Hbanoticus\n",
       "1922530       710       Hppii\n",
       "1937158       716          Ma"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol2_char_df[(vol2_char_df['potential_epithet_match'] == True) & (vol2_char_df['word'].apply(potential_author_match_epithet_coord))][[\"page_num\", \"word\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1554632</th>\n",
       "      <td>555</td>\n",
       "      <td>Krascheninnikovii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565497</th>\n",
       "      <td>560</td>\n",
       "      <td>Wagenitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566524</th>\n",
       "      <td>560</td>\n",
       "      <td>Fritsch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575185</th>\n",
       "      <td>564</td>\n",
       "      <td>Holub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575488</th>\n",
       "      <td>565</td>\n",
       "      <td>Holub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577207</th>\n",
       "      <td>565</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578956</th>\n",
       "      <td>566</td>\n",
       "      <td>Eichwaldii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579044</th>\n",
       "      <td>566</td>\n",
       "      <td>Schrank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582101</th>\n",
       "      <td>568</td>\n",
       "      <td>Kuntze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583001</th>\n",
       "      <td>568</td>\n",
       "      <td>Kuntze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584639</th>\n",
       "      <td>569</td>\n",
       "      <td>Majoranamaracus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586349</th>\n",
       "      <td>570</td>\n",
       "      <td>Schiman-Czeika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586394</th>\n",
       "      <td>570</td>\n",
       "      <td>Czeika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592464</th>\n",
       "      <td>573</td>\n",
       "      <td>Berth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603606</th>\n",
       "      <td>578</td>\n",
       "      <td>DOteriifolium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608442</th>\n",
       "      <td>580</td>\n",
       "      <td>Turra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609136</th>\n",
       "      <td>580</td>\n",
       "      <td>Moretti,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610970</th>\n",
       "      <td>581</td>\n",
       "      <td>Fischer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611185</th>\n",
       "      <td>581</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         page_num               word\n",
       "1554632       555  Krascheninnikovii\n",
       "1565497       560          Wagenitz \n",
       "1566524       560           Fritsch \n",
       "1575185       564              Holub\n",
       "1575488       565             Holub \n",
       "1577207       565                 et\n",
       "1578956       566         Eichwaldii\n",
       "1579044       566           Schrank \n",
       "1582101       568            Kuntze \n",
       "1583001       568             Kuntze\n",
       "1584639       569    Majoranamaracus\n",
       "1586349       570     Schiman-Czeika\n",
       "1586394       570             Czeika\n",
       "1592464       573             Berth.\n",
       "1603606       578      DOteriifolium\n",
       "1608442       580             Turra \n",
       "1609136       580           Moretti,\n",
       "1610970       581           Fischer \n",
       "1611185       581                non"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol3_char_df[(vol3_char_df['potential_epithet_match'] == True) & (vol3_char_df['word'].apply(potential_author_match_epithet_coord))][[\"page_num\", \"word\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flag if we had 2 genus in the same line or 1 or more genus + 1 or more epithet on the same line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
