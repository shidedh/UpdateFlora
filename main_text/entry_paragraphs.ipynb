{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cProfile import label #?not sure\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import difflib \n",
    "from fuzzywuzzy import process\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import fitz\n",
    "\n",
    "from functools import reduce\n",
    "from fitz.utils import getColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_path = '../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 1.pdf'\n",
    "vol2_path = '../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 2.pdf'\n",
    "vol3_path = '../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 3.pdf'\n",
    "\n",
    "vol1_doc = fitz.open(vol1_path)\n",
    "vol2_doc = fitz.open(vol2_path)\n",
    "vol3_doc = fitz.open(vol3_path)\n",
    "\n",
    "vol1_pages = [vol1_doc[i] for i in range(vol1_doc.page_count)]\n",
    "vol2_pages = [vol2_doc[i] for i in range(vol2_doc.page_count)]\n",
    "vol3_pages = [vol3_doc[i] for i in range(vol3_doc.page_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_char_df = pd.read_pickle(\"../input/char_df/vol1_df.pkl\")\n",
    "vol2_char_df = pd.read_pickle(\"../input/char_df/vol2_df.pkl\")\n",
    "vol3_char_df = pd.read_pickle(\"../input/char_df/vol3_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mout. index: \n",
    "vol1_index_path = '../output/local/index_output/vol1_nonitalics.csv'\n",
    "vol2_index_path = '../output/local/index_output/vol2_nonitalics.csv'\n",
    "vol3_index_path = '../output/local/index_output/vol3_nonitalics.csv'\n",
    "\n",
    "vol1_index_df = pd.read_csv(vol1_index_path)\n",
    "vol2_index_df = pd.read_csv(vol2_index_path)\n",
    "vol3_index_df = pd.read_csv(vol3_index_path)\n",
    "\n",
    "#changing name of columns of mout. indecies \n",
    "vol1_index_df.rename(columns={'closest_genus': 'mouterde_genus', 'closest_epithet': 'mouterde_epithet', 'authors':'mouterde_author', 'closest_infra_name':'mouterde_infra'}, inplace=True)\n",
    "vol2_index_df.rename(columns={'closest_genus': 'mouterde_genus', 'closest_epithet': 'mouterde_epithet', 'authors':'mouterde_author', 'closest_infra_name':'mouterde_infra'}, inplace=True)\n",
    "vol3_index_df.rename(columns={'closest_genus': 'mouterde_genus', 'closest_epithet': 'mouterde_epithet', 'authors':'mouterde_author', 'closest_infra_name':'mouterde_infra'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_word_df = vol1_char_df.loc[:, ['vol_num', 'page_num', \n",
    "                                    'block_num', 'block_num_absolute', 'block_bbox',\n",
    "                                    'line_num', 'line_wmode', 'line_dir', 'line_bbox', \n",
    "                                    'span_num', 'span_size', 'span_flags', 'span_font', 'span_color', 'span_ascender', 'span_descender', 'span_origin', 'span_bbox', \n",
    "                                    'word_num', 'word','word_bbox', 'pruned_word', 'pruned_word_bbox']].drop_duplicates()\n",
    "\n",
    "vol2_word_df = vol2_char_df.loc[:, ['vol_num', 'page_num', \n",
    "                                    'block_num', 'block_num_absolute', 'block_bbox',\n",
    "                                    'line_num', 'line_wmode', 'line_dir', 'line_bbox', \n",
    "                                    'span_num', 'span_size', 'span_flags', 'span_font', 'span_color', 'span_ascender', 'span_descender', 'span_origin', 'span_bbox', \n",
    "                                    'word_num', 'word','word_bbox', 'pruned_word', 'pruned_word_bbox']].drop_duplicates()\n",
    "\n",
    "vol3_word_df = vol3_char_df.loc[:, ['vol_num', 'page_num', \n",
    "                                    'block_num', 'block_num_absolute', 'block_bbox',\n",
    "                                    'line_num', 'line_wmode', 'line_dir', 'line_bbox', \n",
    "                                    'span_num', 'span_size', 'span_flags', 'span_font', 'span_color', 'span_ascender', 'span_descender', 'span_origin', 'span_bbox', \n",
    "                                    'word_num', 'word','word_bbox', 'pruned_word', 'pruned_word_bbox']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of genera from index -- uppercased to match main text pattern\n",
    "vol1_genera = vol1_index_df[vol1_index_df['taxon_rank'] == 'genus']['mouterde_genus'].str.upper().tolist()\n",
    "\n",
    "#list of species binomial from main text\n",
    "vol1_species_temp_df = vol1_index_df[(vol1_index_df['taxon_rank'] == 'epithet') & (~vol1_index_df['mouterde_genus'].isna())]\n",
    "vol1_species_binomial_list = list(zip(vol1_species_temp_df['mouterde_genus'], vol1_species_temp_df['mouterde_epithet']))\n",
    "vol1_species = list(map(lambda x: f\"{x[0]} {x[1]}\", vol1_species_binomial_list))\n",
    "vol1_species_abriviation = list(map(lambda x: f\"{x[0][0]}. {x[1]}\", vol1_species_binomial_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_italic(flags):\n",
    "    return flags & 2 ** 1 != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_words_flagged(df, n, inplace = True):\n",
    "    #assumes n > 1\n",
    "    out_words_col = f\"{n}_words\"\n",
    "    out_flags_col = f\"{n}_flags\"\n",
    "    \n",
    "    line_group_cols = ['vol_num', 'page_num', \n",
    "                       'block_num', 'block_num_absolute', 'block_bbox', \n",
    "                       'line_num', 'line_wmode', 'line_dir', 'line_bbox']\n",
    "    \n",
    "    n_words_lists = [None for i in range(n)]\n",
    "    n_words_flags = [None for i in range(n)]\n",
    "\n",
    "    n_words_lists[0] = df['word']\n",
    "    n_words_flags[0] = df['span_flags']\n",
    "\n",
    "    for i in range(1, n):\n",
    "        n_words_lists[i] = df.groupby(line_group_cols)['word'].shift(-i, fill_value=\"\")\n",
    "        n_words_flags[i] = df.groupby(line_group_cols)['span_flags'].shift(-i, fill_value=0)\n",
    "\n",
    "    zip_n_words = list(zip(*n_words_lists))\n",
    "    n_word_string = list(map(lambda n_word_list : \" \".join(n_word_list), zip_n_words))\n",
    "\n",
    "    zip_n_flags = list(zip(*n_words_flags))\n",
    "    combine_flags = list(map(lambda flag_list : reduce(lambda x, y: x | y, flag_list), zip_n_flags))\n",
    "    \n",
    "    if inplace == True:\n",
    "        df[out_words_col] = n_word_string\n",
    "        df[out_flags_col] = combine_flags\n",
    "    return n_word_string, combine_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_word_df = pd.read_pickle(\"../input/desc_box_df/vol1_desc_df_v2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vol_num', 'page_num', 'block_num', 'block_num_absolute', 'block_bbox',\n",
       "       'line_num', 'line_wmode', 'line_dir', 'line_bbox', 'span_num',\n",
       "       'span_size', 'span_flags', 'span_font', 'span_color', 'span_ascender',\n",
       "       'span_descender', 'span_origin', 'span_bbox', 'word_num', 'word',\n",
       "       'word_bbox', 'pruned_word', 'pruned_word_bbox', '1_words', '1_flags',\n",
       "       '1_words_match', '1_words_match_score', '2_words', '2_flags',\n",
       "       '2_words_match', '2_words_match_score', '3_words', '3_flags',\n",
       "       '3_words_match', '3_words_match_score', '4_words', '4_flags',\n",
       "       '4_words_match', '4_words_match_score', 'line_id', 'is_page_title',\n",
       "       'page_title_mean_y', 'is_title_line', 'section_break', 'section_id',\n",
       "       'section_start_y', 'section_bbox', 'binom_section'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol1_word_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_binomial = ((~(vol1_word_df['1_flags'].apply(is_italic)) & (vol1_word_df['1_words_match_score'] > 0.85)) | \n",
    "               (~(vol1_word_df['2_flags'].apply(is_italic)) & (vol1_word_df['2_words_match_score'] > 0.85)) | \n",
    "               (~(vol1_word_df['3_flags'].apply(is_italic)) & (vol1_word_df['3_words_match_score'] > 0.85)) | \n",
    "               (~(vol1_word_df['1_flags'].apply(is_italic)) & (vol1_word_df['1_words_match_score'] > 0.85))) \n",
    "binom_page_num = vol1_word_df[(is_binomial)]['page_num']\n",
    "binom_block_num = vol1_word_df[(is_binomial)]['block_num']\n",
    "binom_line_num = vol1_word_df[(is_binomial)]['line_num']\n",
    "binom_id = list(zip(binom_page_num, binom_block_num, binom_line_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_char_df['line_id'] = vol1_char_df.apply(lambda r : (r['page_num'], r['block_num'], r['line_num']), axis = 1)\n",
    "vol1_char_binom_df = vol1_char_df[vol1_char_df['line_id'].isin(binom_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.925233924113611"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom_char_width = vol1_char_binom_df.groupby('line_id')['char_bbox'].transform(lambda x: x.apply(lambda y: y[2] - y[0])).mean()\n",
    "binom_char_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 642/642 [00:06<00:00, 97.81it/s]\n"
     ]
    }
   ],
   "source": [
    "num_pages = vol1_word_df['page_num'].max() + 1\n",
    "for page_num in tqdm(range(num_pages)):\n",
    "    vol1_word_df.loc[vol1_word_df['page_num'] == page_num, 'mean_binom_x0'] = vol1_word_df[(vol1_word_df['page_num'] == page_num) & (vol1_word_df['line_id'].isin(binom_id))]['line_bbox'].apply(lambda x : x[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314517/314517 [00:01<00:00, 217661.06it/s]\n"
     ]
    }
   ],
   "source": [
    "accepted_error = (binom_char_width)*2.5 #just eyeballing it ...\n",
    "\n",
    "def is_binom_indentation(row):\n",
    "    if abs(row['mean_binom_x0'] - (row['line_bbox'][0])) < accepted_error:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "vol1_word_df['is_binom_indentation'] = vol1_word_df.progress_apply(is_binom_indentation, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          False\n",
       "6          False\n",
       "14         False\n",
       "16         False\n",
       "20         False\n",
       "           ...  \n",
       "1761774     True\n",
       "1761781     True\n",
       "1761784     True\n",
       "1761788     True\n",
       "1761792     True\n",
       "Name: section_break, Length: 314517, dtype: bool"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol1_word_df['section_break']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314517/314517 [00:01<00:00, 219108.09it/s]\n"
     ]
    }
   ],
   "source": [
    "vol1_word_df['paragraph_id'] = np.nan\n",
    "def paragraph_id(row):\n",
    "    if row['is_binom_indentation'] == True:\n",
    "        return row['line_id']\n",
    "    if row['section_break'] == True:\n",
    "        return row['line_id']\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "vol1_word_df['paragraph_id'] = vol1_word_df.progress_apply(paragraph_id, axis = 1)\n",
    "vol1_word_df['paragraph_id'].ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break down line coords\n",
    "vol1_word_df['line_x0'] = vol1_word_df[\"line_bbox\"].apply(lambda x: x[0])\n",
    "vol1_word_df['line_y0'] = vol1_word_df[\"line_bbox\"].apply(lambda x: x[1])\n",
    "vol1_word_df['line_x1'] = vol1_word_df[\"line_bbox\"].apply(lambda x: x[2])\n",
    "vol1_word_df['line_y1'] = vol1_word_df[\"line_bbox\"].apply(lambda x: x[3])\n",
    "\n",
    "#sections_coords: \n",
    "vol1_word_df[\"paragraph_x0\"] = vol1_word_df.groupby(['page_num', 'paragraph_id'])['line_x0'].transform('min')\n",
    "vol1_word_df[\"paragraph_y0\"] = vol1_word_df.groupby(['page_num', 'paragraph_id'])['line_y0'].transform('min')\n",
    "#vol1_word_df[\"section_y0\"] = vol1_word_df[['section_y0_all','section_y0_all']].max(axis=1)\n",
    "vol1_word_df[\"paragraph_x1\"] = vol1_word_df.groupby(['page_num', 'paragraph_id'])['line_x1'].transform('max')\n",
    "vol1_word_df[\"paragraph_y1\"] = vol1_word_df.groupby(['page_num', 'paragraph_id'])['line_y1'].transform('max')\n",
    "\n",
    "#section_bbox:\n",
    "vol1_word_df[\"paragraph_bbox\"] = vol1_word_df.apply(lambda r: (r[\"paragraph_x0\"], r[\"paragraph_y0\"], r[\"paragraph_x1\"], r[\"paragraph_y1\"]), axis = 1)\n",
    "\n",
    "#drop extra cols:\n",
    "vol1_word_df.drop(columns= [\"line_x0\", \"line_y0\", \"line_x1\", \"line_y1\", \"paragraph_x0\", \"paragraph_y0\", \"paragraph_x1\", \"paragraph_y1\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 642/642 [00:03<00:00, 178.13it/s]\n"
     ]
    }
   ],
   "source": [
    "vol1_doc = fitz.open(vol1_path)\n",
    "vol2_doc = fitz.open(vol2_path)\n",
    "vol3_doc = fitz.open(vol3_path)\n",
    "\n",
    "for page_num in tqdm(range(num_pages)):\n",
    "    section_groups = vol1_word_df[vol1_word_df['page_num'] == page_num].groupby('section_id')\n",
    "    page = vol1_doc[page_num]\n",
    "    colors = [getColor(\"plum\"), getColor(\"orchid4\")]\n",
    "    \n",
    "    paragraph_groups = vol1_word_df[vol1_word_df['page_num'] == page_num].groupby('paragraph_id')\n",
    "    for name, paragraph in paragraph_groups:\n",
    "        i = 0\n",
    "        paragraph_id = paragraph.iloc[0]['paragraph_id']\n",
    "        paragraph_section_id = paragraph.iloc[0]['section_id']\n",
    "        paragraph_bbox = paragraph.iloc[0]['paragraph_bbox']\n",
    "        is_L_loc = paragraph.iloc[0]['word'].lower() in [\"l.\", \"l\"]\n",
    "        is_S_loc = paragraph.iloc[0]['word'].lower() in [\"s.\", \"s\"]\n",
    "        \n",
    "        c = getColor(\"lightgray\")\n",
    "        if is_L_loc: \n",
    "            c = getColor(\"lightblue\")\n",
    "        if is_S_loc:\n",
    "            c = getColor(\"pink\")\n",
    "        if paragraph_section_id in binom_id:\n",
    "            r_box = fitz.Rect(paragraph_bbox)\n",
    "            annot_rect = page.add_rect_annot(r_box)\n",
    "            annot_rect.set_colors({\"stroke\":c})\n",
    "            annot_rect.update()\n",
    "            i += 1\n",
    "        c = getColor(\"lightgray\")\n",
    "\n",
    "    for name, section in section_groups:\n",
    "        section_id = section.iloc[0]['section_id']\n",
    "        section_bbox = section.iloc[0]['section_bbox']\n",
    "        if section_id in binom_id:\n",
    "            r_box = fitz.Rect(section_bbox)\n",
    "            #r_box.set_stroke_color(stroke=getColor(\"violetred4\"))\n",
    "            annot_rect = page.add_rect_annot(r_box)\n",
    "            annot_rect.set_colors({\"stroke\": getColor(\"violetred4\")})\n",
    "            annot_rect.update()\n",
    "\n",
    "marked_epithet_fname = \"../output/local/main_text/binom_sections_paragraphs_LS_v1.pdf\"\n",
    "vol1_doc.save(marked_epithet_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### current problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page 110 -- no binom in page -- not catagorizing paragraphs as expected / hoped\n",
    "page 275 -- above ^ \n",
    "page 97 -- L. et S. \n",
    "page 107 -- 2.5 error margin seems too wide ... \n",
    "page 127 -- Phalaris caerulescens -> . L. Ct. ....\n",
    "what to do with stuff like 153 \n",
    "how important is italics entries being in good entries (eg 160)\n",
    "\n",
    "265 is being picked up correctly which is *happy*\n",
    "271 sy. Sy. «Tell près de Zraikiyé, au nord de Sanamein», décembre 1954 (Hauran). Legit PABOT. Typus: Herbier Mouterde P 326\n",
    "\n",
    "real issue : 276 : start of binomial issue: \"'-^ Juncus effusus \n",
    "-- possible solution: for binomial x0, go based on word x0 for part that is the  \n",
    "                      binomial matching part not the line match\n",
    "-- possible solution for pages without binomials: add a column to keep track of distance \n",
    "                                                  between bbox and binomial name x0 (first solve above) and take the average of that value as binomial x0 then add that to the desired page's section bbox x0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vol_num', 'page_num', 'block_num', 'block_num_absolute', 'block_bbox',\n",
       "       'line_num', 'line_wmode', 'line_dir', 'line_bbox', 'span_num',\n",
       "       'span_size', 'span_flags', 'span_font', 'span_color', 'span_ascender',\n",
       "       'span_descender', 'span_origin', 'span_bbox', 'word_num', 'word',\n",
       "       'word_bbox', 'pruned_word', 'pruned_word_bbox', '1_words', '1_flags',\n",
       "       '1_words_match', '1_words_match_score', '2_words', '2_flags',\n",
       "       '2_words_match', '2_words_match_score', '3_words', '3_flags',\n",
       "       '3_words_match', '3_words_match_score', '4_words', '4_flags',\n",
       "       '4_words_match', '4_words_match_score', 'line_id', 'is_page_title',\n",
       "       'page_title_mean_y', 'is_title_line', 'section_break', 'section_id',\n",
       "       'section_start_y', 'section_bbox', 'binom_section', 'mean_binom_x0',\n",
       "       'is_binom_indentation', 'paragraph_id', 'paragraph_bbox'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol1_word_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               AUTRES\n",
       "6             OUVRAGES\n",
       "14                  DU\n",
       "16                MÊME\n",
       "20              AUTEUR\n",
       "              ...     \n",
       "1761774        JUILLET\n",
       "1761781            MIL\n",
       "1761784           NEUF\n",
       "1761788           CENT\n",
       "1761792    SOIXANTESIX\n",
       "Name: pruned_word, Length: 314517, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol1_word_df['pruned_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1), (0.6784313725490196, 0.8470588235294118, 0.9019607843137255)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[getColor(\"darkpink\"), getColor(\"lightblue\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAIAAACRXR/mAAAATklEQVR4nO3OMQHAIBAAsVL/7t4QBlhugiFRkDUz33v+24EzrUKr0Cq0Cq1Cq9AqtAqtQqvQKrQKrUKr0Cq0Cq1Cq9AqtAqtQqvQKrSKDcxIAt0kCKH4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=50x50>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Create a new image with a solid color\n",
    "width = 50\n",
    "height = 50\n",
    "colors = [getColor(\"pink\"), getColor(\"lightblue\"),  getColor(\"lightgray\")]\n",
    "for color_1 in colors:\n",
    "    color_255 = tuple(int(c_val*255) for c_val in color_1) # red color\n",
    "    image = Image.new(\"RGB\", (width, height), color_255)\n",
    "    image.show()\n",
    "# Display the image\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
